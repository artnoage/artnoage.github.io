  <section id="Publications">
<div class="container">
  <h3>Available topics</h3>
  <div class="panel panel-default">
    <div class="panel-body">
		<h5>
			&nbsp;&nbsp;<strong>  Training GANs with general optimal transport cost structures.</strong>
		</h5>
      <article>
        <p>
            <em>Difficulty &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </em> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i>
            &nbsp;&nbsp;
        </p>
        <p>
            <em>Independence&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </em>  <i class="fa fa-star"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i>
            &nbsp;&nbsp;
        </p>
         <p>
            <em>Likelihood of novel results&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
           </em>  <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i>
            &nbsp;&nbsp;
        </p>
    </article>
    <article>
      Soft prequisites : Know how to build and train a NN in tensorflow or Keras, Basic probability distribution theory.
      </article>
      <article>
        Bonus for having knowledge in: GAN architectures.
      </article>
		<h5>
			&nbsp;&nbsp;<strong> Solving POMDPs on continuous state spaces using methods from computational geometry.</strong>
		</h5>
      <article>
        <p>In the classical theory of finite time Markov Decision Processes (MDPs), the objective is to optimize the expected value <span class="math display">\[J_N(x_0,\pi) = \mathbb{E}_{x_{0}}^{\pi}\left[\sum_{n=0}^{N-1}C(X_{n},A_{n})\right],\]</span> after a number of <span class="math inline">\(N\)</span> discrete time steps. The expected value <span class="math inline">\(J_N\)</span> is generated by a random process <span class="math inline">\((X_n)\)</span>, where <span class="math inline">\(X_n\)</span> denotes in which state the process is situated at time <span class="math inline">\(n \in \mathbb{N}\)</span>. This process is controlled via a series of actions <span class="math inline">\((A_n)\)</span>, according to a policy <span class="math inline">\(\pi\)</span>, that changes the underlying state transition probabilities <span class="math inline">\(P(X_{n+1}|X_n,A_n)\)</span> of <span class="math inline">\((X_n)\)</span>. The inclusion of risk-sensitivity and partial observability are natural extensions to this model with applications in various fields such as finance or engineering.</p>
<p>In classical MDPs, one makes the assumption that the controlled process takes values on a set of states which is always accessible to the controller. However, in several real-life applications, the real state is not directly observable&mdash;but a secondary information, dependent on the state, can be observed. Partially Observable Markov Decision Processes (POMDPs) are a generalization of MDPs towards incomplete information about the current state. POMDPs extend the notion of MDPs by a set of observations <span class="math inline">\(\mathcal{Y}\)</span> and a set of conditional observation probabilities <span class="math inline">\(Q(y|s)\)</span> given the 'hidden' state <span class="math inline">\(s \in \mathcal{S}\)</span>.</p>
		  <p>In spite of knowing the existence a theoretical solution, POMDPs were notoriously difficult to solve in practice. In the case where the underlying space is finite, a fast algorithm called point-based value iteration was designed to overcome this numerical difficulty. This algorithm is mainly based on the property first observed by Smallwood, that the optimal solution can be arbitrarily well approximated if one exploits properties of the dual space. Many algorithms based on Smallwood's idea followed. The topic of this thesis is to investigate one of them, which requires some knowledge of computational geometry, with the hope to extend it for cases where the state space is no longer finite  </p>
        <p>
            <em>Difficulty &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </em> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-o"></i>
            &nbsp;&nbsp;
        </p>
        <p>
            <em>Independence&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </em>  <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-o"></i><i class="fa fa-star-o"></i>
            &nbsp;&nbsp;
        </p>
         <p>
            <em>Likelihood of novel results&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
           </em>  <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-o"></i>
            &nbsp;&nbsp;
        </p>
    </article>
    <article>
      Soft prequisites : Linear programming, MDPs
      </article>
      <article>
        Bonus for having knowledge in: Computetional geometry, Duality in vecor spaces.
      </article>
    </div>
  </div>
  
</div>
</section>



 